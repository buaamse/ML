{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "separate-assurance",
   "metadata": {},
   "source": [
    "回归算法是一种有监督学习算法，用来建立自变量X和观测变量Y之间的映射关系，如果观测变量是离散的，则称其为分类Classification；如果观测变量是连续的，则称其为回归Regression。\n",
    "        回归算法的目的是寻找假设函数hypothesis来最好的拟合给定的数据集。常用的回归算法有：线性回归（Linear Regression）、逻辑回归（Logistic Regression）、多项式回归（Polynomial Regression）、岭回归（Ridge Regression）、LASSO回归（Least Absolute Shrinkage and Selection Operator）、弹性网络（Elastic Net estimators）、逐步回归（Stepwise Regression）等。\n",
    "        线性回归模型试图学得一个线性模型以尽可能准确地预测实值X的输出标记Y。在这个模型中，因变量Y是连续的，自变量X可以是连续或离散的。\n",
    "        在回归分析中，如果只包括一个自变量和一个因变量，且二者关系可用一条直线近似表示，称为一元线性回归分析；如果回归分析中包括两个或两个以上的自变量，且因变量和自变量是线性关系，则称为多元线性回归分析。对于二维空间线性是一条直线，对于三维空间线性是一个平面，对于多维空间线性是一个超平面。\n",
    "\n",
    "1、目标函数：\n",
    "        根据数据特征寻找合适的假设函数hθ(x)，构造适合的损失函数J(θ)。求极大似然估计的最大值，或由极大似然可推导出最小二乘函数，求它的最小值。构造假设函数：\n",
    " ![jupyter](./linear.png)\n",
    " ![jupyter](./linear_1.png)\n",
    " \n",
    "  m：输入数据的条数；n：每条数据的特征个数\n",
    "\n",
    "        ：第i个样本，\n",
    "\n",
    "        ：第i个样本的第j个特征\n",
    "\n",
    "        向量化Vectorization 相比传统的for循环求解更简洁、速度也更快。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gothic-routine",
   "metadata": {},
   "source": [
    "梯度下降算法\n",
    "\n",
    "假设线性函数形式： $$h_\\theta \\left( x \\right)=\\theta_{0}+\\theta_{1}x $$\n",
    "\n",
    "损失函数(又称代价函数或成本函数)：\n",
    " $$J(\\theta)=\\frac{1}{2 m} \\sum_{i=1}^{m}\\left(h_{\\theta}\\left(x^{(i)}\\right)-y^{(i)}\\right)^{2}$$\n",
    "梯度下降是一个用来求函数最小值的算法，将使用梯度下降算法来求出损失函数 $$J(\\theta_{0}, \\theta_{1}) $$的最小值。\n",
    "\n",
    "梯度下降思想：开始时我们随机选择一个参数的组合$$\\left({\\theta_{0}},{\\theta_{1}},......,{\\theta_{n}} \\right)\\$$，计算损失函数，然后我们寻找下一个能让损失函数值下降最多的参数组合。我们持续这么做直到找到一个局部最小值（local minimum），因为我们并没有尝试完所有的参数组合，所以不能确定我们得到的局部最小值是否便是全局最小值（global minimum），选择不同的初始参数组合，可能会找到不同的局部最小值。\n",
    "梯度下降理解：想象你自己正站立在山的这一点上，站立在你想象的公园这座红色山上，在梯度下降算法中，我们要做的就是旋转360度，看看我们的周围，并问自己要在某个方向上，用小碎步尽快下山。这些小碎步需要朝什么方向？如果我们站在山坡上的这一点，你看一下周围，你会发现最佳的下山方向，你再看看周围，然后再一次想想，我应该从什么方向迈着小碎步下山？然后你按照自己的判断又迈出一步，重复上面的步骤，从这个新的点，你环顾四周，并决定从什么方向将会最快下山，然后又迈进了一小步，并依此类推，直到你接近局部最低点的位置。\n",
    "![jupyter](./sgd_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afraid-worcester",
   "metadata": {},
   "source": [
    "(3) 选择具体的模型并进行训练\n",
    "   - **线性回归模型**         \n",
    "   回归这个概念是19世纪80年代由英国统计学家郎西斯.高尔顿在研究父子身高关系提出来的，他发现：在同一族群中，子代的平均身高介于父代的身高以及族群的平均身高之间。具体而言，高个子父亲的儿子的身高有低于其父亲身高的趋势，而矮个子父亲的儿子身高则有高于父亲的身高的趋势。也就是说，子代的身高有向族群平均身高\"平均\"的趋势，这就是统计学上\"回归\"的最初含义。回归分析是一种预测性的建模技术，它研究的是因变量（目标）和自变量（特征）之间的关系。这种技术通常用于预测分析，时间序列模型以及发现变量之间的因果关系。通常使用曲线/线来拟合数据点，目标是使曲线到数据点的距离差异最小。而线性回归就是回归问题中的一种，线性回归假设目标值与特征之间线性相关，即满足一个多元一次方程。通过构建损失函数，来求解损失函数最小时的参数w ：                                                \n",
    "   假设：数据集$D = \\{(x_1,y_1),...,(x_N,y_N) \\}$，$x_i \\in R^p,y_i \\in R,i = 1,2,...,N$，$X = (x_1,x_2,...,x_N)^T,Y=(y_1,y_2,...,y_N)^T$                        \n",
    "   假设X和Y之间存在线性关系，模型的具体形式为$\\hat{y}=f(w) =w^Tx$           \n",
    "   ![jupyter](./1.4.png)      \n",
    "   (a) 最小二乘估计：                 \n",
    "   我们需要衡量真实值$y_i$与线性回归模型的预测值$w^Tx_i$之间的差距，在这里我们和使用二范数的平方和L(w)来描述这种差距，即：                      \n",
    "   $$\n",
    "   L(w) = \\sum\\limits_{i=1}^{N}||w^Tx_i-y_i||_2^2=\\sum\\limits_{i=1}^{N}(w^Tx_i-y_i)^2 = (w^TX^T-Y^T)(w^TX^T-Y^T)^T = w^TX^TXw - 2w^TX^TY+YY^T\\\\\n",
    "   因此，我们需要找到使得L(w)最小时对应的参数w，即：\\\\\n",
    "   \\hat{w} = argmin\\;L(w)\\\\\n",
    "   为了达到求解最小化L(w)问题，我们应用高等数学的知识，使用求导来解决这个问题： \\\\\n",
    "   \\frac{\\partial L(w)}{\\partial w} = 2X^TXw-2X^TY = 0,因此： \\\\\n",
    "   \\hat{w} = (X^TX)^{-1}X^TY\n",
    "   $$                   \n",
    "   (b) 几何解释：                \n",
    "   在线性代数中，我们知道两个向量a和b相互垂直可以得出：$<a,b> = a.b = a^Tb = 0$,而平面X的法向量为Y-Xw，与平面X互相垂直，因此：$X^T(Y-Xw) = 0$，即：$w = (X^TX)^{-1}X^TY$                             \n",
    "   ![jupyter](./1.5.png)               \n",
    "   (c) 概率视角：       \n",
    "   假设噪声$\\epsilon \\backsim N(0,\\sigma^2),y=f(w)+\\epsilon=w^Tx+\\epsilon$，因此：$y|x_i,w ~ N(w^Tx,\\sigma^2)$          \n",
    "   我们使用极大似然估计MLE对参数w进行估计：       \n",
    "   $$\n",
    "   L(w) = log\\;P(Y|X;w) = log\\;\\prod_{i=1}^N P(y_i|x_i;w) = \\sum\\limits_{i=1}^{N} log\\; P(y_i|x_i;w)\\\\\n",
    "    = \\sum\\limits_{i=1}^{N}log(\\frac{1}{\\sqrt{2\\pi \\sigma}}exp(-\\frac{(y_i-w^Tx_i)^2}{2\\sigma^2})) = \\sum\\limits_{i=1}^{N}[log(\\frac{1}{\\sqrt{2\\pi}\\sigma})-\\frac{1}{2\\sigma^2}(y_i-w^Tx_i)^2] \\\\\n",
    "    argmax_w L(w) = argmin_w[l(w) = \\sum\\limits_{i = 1}^{N}(y_i-w^Tx_i)^2]\\\\\n",
    "    因此：线性回归的最小二乘估计<==>噪声\\epsilon\\backsim N(0,\\sigma^2)的极大似然估计\n",
    "   $$\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "literary-model",
   "metadata": {},
   "source": [
    "下面，我们使用sklearn的线性回归实例来演示：                   \n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html#sklearn.linear_model.LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bound-incentive",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 引入相关科学计算包\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "plt.style.use(\"ggplot\")      \n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "serial-machinery",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "      <th>Price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1.0</td>\n",
       "      <td>296.0</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "      <td>21.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "      <td>34.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "      <td>33.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "      <td>36.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.02985</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.430</td>\n",
       "      <td>58.7</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.12</td>\n",
       "      <td>5.21</td>\n",
       "      <td>28.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.08829</td>\n",
       "      <td>12.5</td>\n",
       "      <td>7.87</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.524</td>\n",
       "      <td>6.012</td>\n",
       "      <td>66.6</td>\n",
       "      <td>5.5605</td>\n",
       "      <td>5.0</td>\n",
       "      <td>311.0</td>\n",
       "      <td>15.2</td>\n",
       "      <td>395.60</td>\n",
       "      <td>12.43</td>\n",
       "      <td>22.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.14455</td>\n",
       "      <td>12.5</td>\n",
       "      <td>7.87</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.524</td>\n",
       "      <td>6.172</td>\n",
       "      <td>96.1</td>\n",
       "      <td>5.9505</td>\n",
       "      <td>5.0</td>\n",
       "      <td>311.0</td>\n",
       "      <td>15.2</td>\n",
       "      <td>396.90</td>\n",
       "      <td>19.15</td>\n",
       "      <td>27.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.21124</td>\n",
       "      <td>12.5</td>\n",
       "      <td>7.87</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.524</td>\n",
       "      <td>5.631</td>\n",
       "      <td>100.0</td>\n",
       "      <td>6.0821</td>\n",
       "      <td>5.0</td>\n",
       "      <td>311.0</td>\n",
       "      <td>15.2</td>\n",
       "      <td>386.63</td>\n",
       "      <td>29.93</td>\n",
       "      <td>16.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.17004</td>\n",
       "      <td>12.5</td>\n",
       "      <td>7.87</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.524</td>\n",
       "      <td>6.004</td>\n",
       "      <td>85.9</td>\n",
       "      <td>6.5921</td>\n",
       "      <td>5.0</td>\n",
       "      <td>311.0</td>\n",
       "      <td>15.2</td>\n",
       "      <td>386.71</td>\n",
       "      <td>17.10</td>\n",
       "      <td>18.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.22489</td>\n",
       "      <td>12.5</td>\n",
       "      <td>7.87</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.524</td>\n",
       "      <td>6.377</td>\n",
       "      <td>94.3</td>\n",
       "      <td>6.3467</td>\n",
       "      <td>5.0</td>\n",
       "      <td>311.0</td>\n",
       "      <td>15.2</td>\n",
       "      <td>392.52</td>\n",
       "      <td>20.45</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.11747</td>\n",
       "      <td>12.5</td>\n",
       "      <td>7.87</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.524</td>\n",
       "      <td>6.009</td>\n",
       "      <td>82.9</td>\n",
       "      <td>6.2267</td>\n",
       "      <td>5.0</td>\n",
       "      <td>311.0</td>\n",
       "      <td>15.2</td>\n",
       "      <td>396.90</td>\n",
       "      <td>13.27</td>\n",
       "      <td>18.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.09378</td>\n",
       "      <td>12.5</td>\n",
       "      <td>7.87</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.524</td>\n",
       "      <td>5.889</td>\n",
       "      <td>39.0</td>\n",
       "      <td>5.4509</td>\n",
       "      <td>5.0</td>\n",
       "      <td>311.0</td>\n",
       "      <td>15.2</td>\n",
       "      <td>390.50</td>\n",
       "      <td>15.71</td>\n",
       "      <td>21.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.62976</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>5.949</td>\n",
       "      <td>61.8</td>\n",
       "      <td>4.7075</td>\n",
       "      <td>4.0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>396.90</td>\n",
       "      <td>8.26</td>\n",
       "      <td>20.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.63796</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.096</td>\n",
       "      <td>84.5</td>\n",
       "      <td>4.4619</td>\n",
       "      <td>4.0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>380.02</td>\n",
       "      <td>10.26</td>\n",
       "      <td>18.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.62739</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>5.834</td>\n",
       "      <td>56.5</td>\n",
       "      <td>4.4986</td>\n",
       "      <td>4.0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>395.62</td>\n",
       "      <td>8.47</td>\n",
       "      <td>19.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1.05393</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>5.935</td>\n",
       "      <td>29.3</td>\n",
       "      <td>4.4986</td>\n",
       "      <td>4.0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>386.85</td>\n",
       "      <td>6.58</td>\n",
       "      <td>23.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.78420</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>5.990</td>\n",
       "      <td>81.7</td>\n",
       "      <td>4.2579</td>\n",
       "      <td>4.0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>386.75</td>\n",
       "      <td>14.67</td>\n",
       "      <td>17.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.80271</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>5.456</td>\n",
       "      <td>36.6</td>\n",
       "      <td>3.7965</td>\n",
       "      <td>4.0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>288.99</td>\n",
       "      <td>11.69</td>\n",
       "      <td>20.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.72580</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>5.727</td>\n",
       "      <td>69.5</td>\n",
       "      <td>3.7965</td>\n",
       "      <td>4.0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>390.95</td>\n",
       "      <td>11.28</td>\n",
       "      <td>18.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       CRIM    ZN  INDUS  CHAS    NOX     RM    AGE     DIS  RAD    TAX  \\\n",
       "0   0.00632  18.0   2.31   0.0  0.538  6.575   65.2  4.0900  1.0  296.0   \n",
       "1   0.02731   0.0   7.07   0.0  0.469  6.421   78.9  4.9671  2.0  242.0   \n",
       "2   0.02729   0.0   7.07   0.0  0.469  7.185   61.1  4.9671  2.0  242.0   \n",
       "3   0.03237   0.0   2.18   0.0  0.458  6.998   45.8  6.0622  3.0  222.0   \n",
       "4   0.06905   0.0   2.18   0.0  0.458  7.147   54.2  6.0622  3.0  222.0   \n",
       "5   0.02985   0.0   2.18   0.0  0.458  6.430   58.7  6.0622  3.0  222.0   \n",
       "6   0.08829  12.5   7.87   0.0  0.524  6.012   66.6  5.5605  5.0  311.0   \n",
       "7   0.14455  12.5   7.87   0.0  0.524  6.172   96.1  5.9505  5.0  311.0   \n",
       "8   0.21124  12.5   7.87   0.0  0.524  5.631  100.0  6.0821  5.0  311.0   \n",
       "9   0.17004  12.5   7.87   0.0  0.524  6.004   85.9  6.5921  5.0  311.0   \n",
       "10  0.22489  12.5   7.87   0.0  0.524  6.377   94.3  6.3467  5.0  311.0   \n",
       "11  0.11747  12.5   7.87   0.0  0.524  6.009   82.9  6.2267  5.0  311.0   \n",
       "12  0.09378  12.5   7.87   0.0  0.524  5.889   39.0  5.4509  5.0  311.0   \n",
       "13  0.62976   0.0   8.14   0.0  0.538  5.949   61.8  4.7075  4.0  307.0   \n",
       "14  0.63796   0.0   8.14   0.0  0.538  6.096   84.5  4.4619  4.0  307.0   \n",
       "15  0.62739   0.0   8.14   0.0  0.538  5.834   56.5  4.4986  4.0  307.0   \n",
       "16  1.05393   0.0   8.14   0.0  0.538  5.935   29.3  4.4986  4.0  307.0   \n",
       "17  0.78420   0.0   8.14   0.0  0.538  5.990   81.7  4.2579  4.0  307.0   \n",
       "18  0.80271   0.0   8.14   0.0  0.538  5.456   36.6  3.7965  4.0  307.0   \n",
       "19  0.72580   0.0   8.14   0.0  0.538  5.727   69.5  3.7965  4.0  307.0   \n",
       "\n",
       "    PTRATIO       B  LSTAT  Price  \n",
       "0      15.3  396.90   4.98   24.0  \n",
       "1      17.8  396.90   9.14   21.6  \n",
       "2      17.8  392.83   4.03   34.7  \n",
       "3      18.7  394.63   2.94   33.4  \n",
       "4      18.7  396.90   5.33   36.2  \n",
       "5      18.7  394.12   5.21   28.7  \n",
       "6      15.2  395.60  12.43   22.9  \n",
       "7      15.2  396.90  19.15   27.1  \n",
       "8      15.2  386.63  29.93   16.5  \n",
       "9      15.2  386.71  17.10   18.9  \n",
       "10     15.2  392.52  20.45   15.0  \n",
       "11     15.2  396.90  13.27   18.9  \n",
       "12     15.2  390.50  15.71   21.7  \n",
       "13     21.0  396.90   8.26   20.4  \n",
       "14     21.0  380.02  10.26   18.2  \n",
       "15     21.0  395.62   8.47   19.9  \n",
       "16     21.0  386.85   6.58   23.1  \n",
       "17     21.0  386.75  14.67   17.5  \n",
       "18     21.0  288.99  11.69   20.2  \n",
       "19     21.0  390.95  11.28   18.2  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 读取boston房价数据，price是因变量\n",
    "from sklearn import datasets\n",
    "boston = datasets.load_boston()     # 返回一个类似于字典的类\n",
    "X = boston.data\n",
    "y = boston.target\n",
    "features = boston.feature_names\n",
    "boston_data = pd.DataFrame(X,columns=features)\n",
    "boston_data[\"Price\"] = y\n",
    "boston_data.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "extreme-sweden",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型系数： [-1.08011358e-01  4.64204584e-02  2.05586264e-02  2.68673382e+00\n",
      " -1.77666112e+01  3.80986521e+00  6.92224640e-04 -1.47556685e+00\n",
      "  3.06049479e-01 -1.23345939e-02 -9.52747232e-01  9.31168327e-03\n",
      " -5.24758378e-01]\n",
      "模型得分： 0.7406426641094095\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model      # 引入线性回归方法\n",
    "lin_reg = linear_model.LinearRegression()       # 创建线性回归的类\n",
    "lin_reg.fit(X,y)        # 输入特征X和因变量y进行训练\n",
    "print(\"模型系数：\",lin_reg.coef_)             # 输出模型的系数\n",
    "print(\"模型得分：\",lin_reg.score(X,y))    # 输出模型的决定系数R^2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "historic-senate",
   "metadata": {},
   "source": [
    "   - **线性回归的推广**                     \n",
    "   在线性回归中，我们假设因变量与特征之间的关系是线性关系，这样的假设使得模型很简单，但是缺点也是显然的，那就是当数据存在非线性关系时，我们使用线性回归模型进行预测会导致预测性能极其低下，因为模型的形式本身是线性的，无法表达数据中的非线性关系。我们一个很自然的想法就是去推广线性回归模型，使得推广后的模型更能表达非线性的关系。                     \n",
    "   (a) 多项式回归：        \n",
    "   为了体现因变量和特征的非线性关系，一个很自然而然的想法就是将标准的线性回归模型：             \n",
    "   $$\n",
    "   y_i = w_0 + w_1x_i + \\epsilon_i\n",
    "   $$               \n",
    "   换成一个多项式函数：          \n",
    "   $$\n",
    "   y_i = w_0 + w_1x_i + w_2x_i^2 + ...+w_dx_i^d + \\epsilon\n",
    "   $$                        \n",
    "   对于多项式的阶数d不能取过大，一般不大于3或者4，因为d越大，多项式曲线就会越光滑，在X的边界处有异常的波动。（图中的边界处的4阶多项式拟合曲线的置信区间(虚线表示置信区间)明显增大，预测效果的稳定性下降。）   \n",
    "\n",
    "   ![jupyter](./1.6.1.png) ![jupyter](./1.6.2.png)                             \n",
    "   \n",
    "   (b) 广义可加模型(GAM)：         \n",
    "   广义可加模型GAM实际上是线性模型推广至非线性模型的一个框架，在这个框架中，每一个变量都用一个非线性函数来代替，但是模型本身保持整体可加性。GAM模型不仅仅可以用在线性回归的推广，还可以将线性分类模型进行推广。具体的推广形式是：          \n",
    "   标准的线性回归模型：           \n",
    "   $$\n",
    "   y_i = w_0 + w_1x_{i1} +...+w_px_{ip} + \\epsilon_i \n",
    "   $$                 \n",
    "   GAM模型框架：                  \n",
    "   $$\n",
    "   y_i = w_0 + \\sum\\limits_{j=1}^{p}f_{j}(x_{ij}) + \\epsilon_i\n",
    "   $$                  \n",
    "   GAM模型的优点与不足：             \n",
    "      - 优点：简单容易操作，能够很自然地推广线性回归模型至非线性模型，使得模型的预测精度有所上升；由于模型本身是可加的，因此GAM还是能像线性回归模型一样把其他因素控制不变的情况下单独对某个变量进行推断，极大地保留了线性回归的易于推断的性质。\n",
    "      - 缺点：GAM模型会经常忽略一些有意义的交互作用，比如某两个特征共同影响因变量，不过GAM还是能像线性回归一样加入交互项$x^{(i)} \\times x^{(j)}$的形式进行建模；但是GAM模型本质上还是一个可加模型，如果我们能摆脱可加性模型形式，可能还会提升模型预测精度，详情请看后面的算法。\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cardiovascular-worcester",
   "metadata": {},
   "source": [
    "(b) GAM模型实例介绍：          \n",
    "安装pygam：pip install pygam               \n",
    "https://github.com/dswah/pyGAM/blob/master/doc/source/notebooks/quick_start.ipynb  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "treated-kruger",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearGAM                                                                                                 \n",
      "=============================================== ==========================================================\n",
      "Distribution:                        NormalDist Effective DoF:                                    103.2423\n",
      "Link Function:                     IdentityLink Log Likelihood:                                 -1589.7653\n",
      "Number of Samples:                          506 AIC:                                             3388.0152\n",
      "                                                AICc:                                            3442.7649\n",
      "                                                GCV:                                               13.7683\n",
      "                                                Scale:                                              8.8269\n",
      "                                                Pseudo R-Squared:                                   0.9168\n",
      "==========================================================================================================\n",
      "Feature Function                  Lambda               Rank         EDoF         P > x        Sig. Code   \n",
      "================================= ==================== ============ ============ ============ ============\n",
      "s(0)                              [0.6]                20           11.3         2.20e-11     ***         \n",
      "s(1)                              [0.6]                20           12.6         8.15e-02     .           \n",
      "s(2)                              [0.6]                20           13.6         2.59e-03     **          \n",
      "s(3)                              [0.6]                20           3.6          2.76e-01                 \n",
      "s(4)                              [0.6]                20           11.3         1.11e-16     ***         \n",
      "s(5)                              [0.6]                20           10.2         1.11e-16     ***         \n",
      "s(6)                              [0.6]                20           10.3         8.22e-01                 \n",
      "s(7)                              [0.6]                20           8.5          4.44e-16     ***         \n",
      "s(8)                              [0.6]                20           3.6          5.96e-03     **          \n",
      "s(9)                              [0.6]                20           3.4          1.33e-09     ***         \n",
      "s(10)                             [0.6]                20           1.8          3.26e-03     **          \n",
      "s(11)                             [0.6]                20           6.4          6.25e-02     .           \n",
      "s(12)                             [0.6]                20           6.6          1.11e-16     ***         \n",
      "intercept                                              1            0.0          2.23e-13     ***         \n",
      "==========================================================================================================\n",
      "Significance codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n",
      "\n",
      "WARNING: Fitting splines and a linear function to a feature introduces a model identifiability problem\n",
      "         which can cause p-values to appear significant when they are not.\n",
      "\n",
      "WARNING: p-values calculated in this manner behave correctly for un-penalized models or models with\n",
      "         known smoothing parameters, but when smoothing parameters have been estimated, the p-values\n",
      "         are typically lower than they should be, meaning that the tests reject the null too readily.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-8-bb049c74476f>:3: UserWarning: KNOWN BUG: p-values computed in this summary are likely much smaller than they should be. \n",
      " \n",
      "Please do not make inferences based on these values! \n",
      "\n",
      "Collaborate on a solution, and stay up to date at: \n",
      "github.com/dswah/pyGAM/issues/163 \n",
      "\n",
      "  gam.summary()\n"
     ]
    }
   ],
   "source": [
    "from pygam import LinearGAM\n",
    "gam = LinearGAM().fit(boston_data[boston.feature_names], y)\n",
    "gam.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "demanding-receipt",
   "metadata": {},
   "source": [
    "(a) 多项式回归实例介绍：                    \n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html?highlight=poly#sklearn.preprocessing.PolynomialFeatures                              \n",
    "sklearn.preprocessing.PolynomialFeatures(degree=2, *, interaction_only=False, include_bias=True, order='C'):               \n",
    "   - 参数：         \n",
    "   degree：特征转换的阶数。                       \n",
    "   interaction_onlyboolean：是否只包含交互项，默认False 。              \n",
    "   include_bias：是否包含截距项，默认True。           \n",
    "   order：str in {‘C’, ‘F’}, default ‘C’，输出数组的顺序。     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "about-liberia",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原始X为：\n",
      " [[0 1]\n",
      " [2 3]\n",
      " [4 5]]\n",
      "2次转化X：\n",
      " [[ 1.  0.  1.  0.  0.  1.]\n",
      " [ 1.  2.  3.  4.  6.  9.]\n",
      " [ 1.  4.  5. 16. 20. 25.]]\n",
      "2次转化X：\n",
      " [[ 1.  0.  1.  0.]\n",
      " [ 1.  2.  3.  6.]\n",
      " [ 1.  4.  5. 20.]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "X_arr = np.arange(6).reshape(3, 2)\n",
    "print(\"原始X为：\\n\",X_arr)\n",
    "\n",
    "poly = PolynomialFeatures(2)\n",
    "print(\"2次转化X：\\n\",poly.fit_transform(X_arr))\n",
    "\n",
    "poly = PolynomialFeatures(interaction_only=True)\n",
    "print(\"2次转化X：\\n\",poly.fit_transform(X_arr))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "instructional-province",
   "metadata": {},
   "source": [
    "   - **回归树：**                                   \n",
    "   基于树的回归方法主要是依据分层和分割的方式将特征空间划分为一系列简单的区域。对某个给定的待预测的自变量，用他所属区域中训练集的平均数或者众数对其进行预测。由于划分特征空间的分裂规则可以用树的形式进行概括，因此这类方法称为决策树方法。决策树由结点(node)和有向边(diredcted edge)组成。结点有两种类型：内部结点(internal node)和叶结点(leaf node)。内部结点表示一个特征或属性，叶结点表示一个类别或者某个值。区域$R_1,R_2$等称为叶节点，将特征空间分开的点为内部节点。                           \n",
    "   ![jupyter](./1.7.png)                     \n",
    "   建立回归树的过程大致可以分为以下两步：          \n",
    "      - 将自变量的特征空间(即$x^{(1)},x^{(2)},x^{(3)},...,x^{(p)}$)的可能取值构成的集合分割成J个互不重叠的区域$R_1,R_2,...,R_j$。        \n",
    "      - 对落入区域$R_j$的每个观测值作相同的预测，预测值等于$R_j$上训练集的因变量的简单算术平均。              \n",
    "   具体来说，就是：             \n",
    "      a.  选择最优切分特征j以及该特征上的最优点s：                \n",
    "      遍历特征j以及固定j后遍历切分点s，选择使得下式最小的(j,s)  $min_{j,s}[min_{c_1}\\sum\\limits_{x_i\\in R_1(j,s)}(y_i-c_1)^2 + min_{c_2}\\sum\\limits_{x_i\\in R_2(j,s)}(y_i-c_2)^2 ]$                           \n",
    "       b. 按照(j,s)分裂特征空间：$R_1(j,s) = \\{x|x^{j} \\le s \\}和R_2(j,s) = \\{x|x^{j} > s \\},\\hat{c}_m = \\frac{1}{N_m}\\sum\\limits_{x \\in R_m(j,s)}y_i,\\;m=1,2$                           \n",
    "       c. 继续调用步骤1，2直到满足停止条件，就是每个区域的样本数小于等于5。        \n",
    "       d. 将特征空间划分为J个不同的区域，生成回归树：$f(x) = \\sum\\limits_{m=1}^{J}\\hat{c}_mI(x \\in R_m)$                \n",
    "       如以下生成的关于运动员在棒球大联盟数据的回归树：             \n",
    "       ![jupyter](./1.8.png)                   \n",
    "    回归树与线性模型的比较：              \n",
    "    线性模型的模型形式与树模型的模型形式有着本质的区别，具体而言，线性回归对模型形式做了如下假定：$f(x) = w_0 + \\sum\\limits_{j=1}^{p}w_jx^{(j)}$，而回归树则是$f(x) = \\sum\\limits_{m=1}^{J}\\hat{c}_mI(x \\in R_m)$。那问题来了，哪种模型更优呢？这个要视具体情况而言，如果特征变量与因变量的关系能很好的用线性关系来表达，那么线性回归通常有着不错的预测效果，拟合效果则优于不能揭示线性结构的回归树。反之，如果特征变量与因变量的关系呈现高度复杂的非线性，那么树方法比传统方法更优。                     \n",
    "    ![jupyter](./1.9.1.png)                        \n",
    "    树模型的优缺点：                 \n",
    "    - 树模型的解释性强，在解释性方面可能比线性回归还要方便。\n",
    "    - 树模型更接近人的决策方式。\n",
    "    - 树模型可以用图来表示，非专业人士也可以轻松解读。\n",
    "    - 树模型可以直接做定性的特征而不需要像线性回归一样哑元化。\n",
    "    - 树模型能很好处理缺失值和异常值，对异常值不敏感，但是这个对线性模型来说却是致命的。\n",
    "    - 树模型的预测准确性一般无法达到其他回归模型的水平，但是改进的方法很多。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "national-armstrong",
   "metadata": {},
   "source": [
    "如果特征变量与因变量的关系能很好的用线性关系来表达，那么线性回归通常有着不错的预测效果，拟合效果则优于不能揭示线性结构的回归树。反之，如果特征变量与因变量的关系呈现高度复杂的非线性，那么树方法比传统方法更优。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "centered-growing",
   "metadata": {},
   "source": [
    "sklearn使用回归树的实例：                \n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html?highlight=tree#sklearn.tree.DecisionTreeRegressor                         \n",
    "sklearn.tree.DecisionTreeRegressor(*, criterion='mse', splitter='best', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=None, random_state=None, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, presort='deprecated', ccp_alpha=0.0）                                                 \n",
    "   - 参数：(列举几个重要的，常用的，详情请看上面的官网)                 \n",
    "   criterion：{“ mse”，“ friedman_mse”，“ mae”}，默认=“ mse”。衡量分割标准的函数 。                      \n",
    "   splitter：{“best”, “random”}, default=”best”。分割方式。                    \n",
    "   max_depth：树的最大深度。               \n",
    "   min_samples_split：拆分内部节点所需的最少样本数，默认是2。                      \n",
    "   min_samples_leaf：在叶节点处需要的最小样本数。默认是1。                    \n",
    "   min_weight_fraction_leaf：在所有叶节点处（所有输入样本）的权重总和中的最小加权分数。如果未提供sample_weight，则样本的权重相等。默认是0。                      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dietary-kernel",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9376307599929274"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor    \n",
    "reg_tree = DecisionTreeRegressor(criterion = \"mse\",min_samples_leaf = 5)\n",
    "reg_tree.fit(X,y)\n",
    "reg_tree.score(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chronic-internet",
   "metadata": {},
   "source": [
    "   - 支持向量机回归(SVR)                                        \n",
    "   在介绍支持向量回归SVR之前，我们先来了解下约束优化的相关知识：          \n",
    "      - 约束优化问题(P)：                   \n",
    "      $$\n",
    "      min f(x)  \\\\\n",
    "      s.t.\\;\\;\\;g_i(x) \\le 0,\\; i=1,2,...,m\\\\\n",
    "      \\;\\;\\;\\;\\; h_j(x) = 0,\\; j=1,2,...,l\n",
    "      $$                         \n",
    "   我们假设$x^*$为满足以上条件的局部最优解，$p^* = f(x^*)$，我们的目的就是要找到$x^*$与$p^*$，满足不等式和等式约束的x集合成为可行域，记作S。\n",
    "   \n",
    "    - KKT条件(最优解的一阶必要条件)          \n",
    "    因为KKT条件是最优化的相关内容，在本次开源学习中并不是重点，因此在这里我用一个更加简单的例子说明KKT条件，严格的证明请参见凸优化相关书籍。                       \n",
    "    在这个例子中，我们考虑：($x^*$为我们的最优解)               \n",
    "    $$\n",
    "    minf(x)\\\\\n",
    "    s.t.\\;g_1(x) \\le 0,\\;x \\in R^n\\\\\n",
    "    \\;\\;\\;g_2(x) \\le 0\\\\\n",
    "    \\;\\;\\;g_3(x) \\le 0\n",
    "    $$\n",
    "    ![jupyter](./1.11.png)\n",
    "      \n",
    "      我们可以看到：$-\\nabla f(x^*)$可以由$\\nabla g_1(x^*)$与$\\nabla g_2(x^*)$线性表出，因此有：$-\\nabla f(x^*) = \\lambda_1 \\nabla g_1(x^*) + \\lambda_2 \\nabla g_2(x^*)$，其中$\\lambda_1,\\lambda_2 \\ge 0$，即：                          \n",
    "      $$\n",
    "      \\nabla f(x^*) + \\lambda_1 \\nabla g_1(x^*) + \\lambda_2 \\nabla g_2(x^*) = 0,\\;\\;\\;其中\\lambda_1,\\lambda_2 \\ge 0\n",
    "      $$                       \n",
    "      我们把没有起作用的约束$g_3(x)$也放到式子里面去，目的也就是为了书写方便，即要求：                  \n",
    "      $$\n",
    "      \\nabla f(x^*) + \\lambda_1 \\nabla g_1(x^*) + \\lambda_2 \\nabla g_2(x^*) + \\lambda_3 \\nabla g_3(x^*)= 0,\\;\\;\\;其中\\lambda_1,\\lambda_2 \\ge 0,\\lambda_3 = 0\n",
    "      $$                      \n",
    "      由于点$x^*$位于方程$g_1(x)=0$与$g_2(x)=0$上，因此：$\\lambda_1 g_1(x^*)  = 0,\\lambda_2  g_2(x^*) = 0 , \\lambda_3  g_3(x^*)= 0$                        \n",
    "      \n",
    "      因此，KKT条件就是：假设$x^*$为最优化问题(P)的局部最优解，且$x^*$ 在某个适当的条件下 ,有：                             \n",
    "      $$\n",
    "     \\nabla f(x^*) + \\sum\\limits_{i=1}^{m}\\lambda_i \\nabla g(x^*) + \\sum\\limits_{j=1}^{l}\\mu_j \\nabla h_j(x^*) = 0(对偶条件)\\\\     \n",
    "      \\lambda_i \\ge 0,\\;i = 1,2,...,m(对偶条件)\\\\\n",
    "      g_i(x^*) \\le 0(原问题条件)\\\\\n",
    "      h_j(x^*) = 0(原问题条件)\\\\\n",
    "      \\lambda_i g(x^*) = 0(互补松弛定理)\n",
    "      $$                              \n",
    " - 对偶理论：               \n",
    "   为什么要引入对偶问题呢？是因为原问题与对偶问题就像是一个问题两个角度去看，如利润最大与成本最低等。有时侯原问题上难以解决，但是在对偶问题上就会变得很简单。再者，任何一个原问题在变成对偶问题后都会变成一个凸优化的问题，这点我们后面会有介绍。下面我们来引入对偶问题：                           \n",
    "   首先，我们的原问题(P)是：\n",
    "      $$\n",
    "      min f(x)  \\\\\n",
    "      s.t.\\;\\;\\;g_i(x) \\le 0,\\; i=1,2,...,m\\\\\n",
    "      \\;\\;\\;\\;\\; h_j(x) = 0,\\; j=1,2,...,l\n",
    "      $$                             \n",
    "      引入拉格朗日函数：$L(x,\\lambda,\\mu) = f(x) + \\sum\\limits_{i=1}^{m}\\lambda_i g_i(x) + \\sum\\limits_{j=1}^{l}\\mu_j h_j(x)$                        \n",
    "      拉格朗日对偶函数：                                  \n",
    "      $$\n",
    "      d(\\lambda,\\mu)  =  min_{x\\in X}\\{ f(x) + \\sum\\limits_{i=1}^{m}\\lambda_i g_i(x) + \\sum\\limits_{j=1}^{l}\\mu_j h_j(x)\\} ,其中X为满足条件的x变量\\\\\n",
    "      \\le min_{x\\in S}\\{ f(x) + \\sum\\limits_{i=1}^{m}\\lambda_i g_i(x) + \\sum\\limits_{j=1}^{l}\\mu_j h_j(x) \\},由于g_i(x) \\le 0,h_j(x) = 0,\\lambda_i \\ge 0 ,其中S为可行域\\\\\n",
    "      \\le min_{x\\in S}\\{f(x) \\}\n",
    "      $$                                   \n",
    "      因此：拉格朗日对偶函数$d(\\lambda,\\mu)$是原问题最优解的函数值$p^*$的下界，即每个不同的$\\lambda$与$\\mu$确定的$d(\\lambda,\\mu)$都是$p^*$的下界，但是我们希望下界越大越好，因为越大就更能接近真实的$p^*$。因此：                               \n",
    "      拉格朗日对偶问题(D)转化为：                 \n",
    "      $$\n",
    "      max_{\\lambda,\\mu}d(\\lambda,\\mu)\\\\\n",
    "      s.t. \\lambda_i \\ge 0,i = 1,2,...,m\\\\\n",
    "      也就是：\\\\\n",
    "      max_{\\lambda \\ge 0,\\mu}\\;min_{x \\in S} L(x,\\lambda,\\mu)\n",
    "      $$                   \n",
    "      我们可以观察到，对偶问题是关于$\\lambda$和$\\mu$的线性函数，因此对偶问题是一个凸优化问题，凸优化问题在最优化理论较为简单。\n",
    "      弱对偶定理：对偶问题(D)的最优解$D^*$一定小于原问题最优解$P^*$，这点在刚刚的讨论得到了充分的证明，一定成立。                   \n",
    "      强对偶定理：对偶问题(D)的最优解$D^*$在一定的条件下等于原问题最优解$P^*$，条件非常多样化且不是唯一的，也就是说这是个开放性的问题，在这里我给出一个最简单的条件，即：$f(x)$与$g_i(x)$为凸函数，$h_j(x)$为线性函数，X是凸集，$x^*$满足KKT条件，那么$D^* = P^*$。 \n",
    "      \n",
    "   \n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spiritual-wilson",
   "metadata": {},
   "source": [
    "sklearn中使用SVR实例：\n",
    "sklearn.svm.SVR(*, kernel='rbf', degree=3, gamma='scale', coef0=0.0, tol=0.001, C=1.0, epsilon=0.1, shrinking=True, cache_size=200, verbose=False, max_iter=-1)\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html?highlight=svr#sklearn.svm.SVR\n",
    "\n",
    "    参数：\n",
    "    kernel：核函数，{‘linear’, ‘poly’, ‘rbf’, ‘sigmoid’, ‘precomputed’}, 默认=’rbf’。(后面会详细介绍)\n",
    "    degree：多项式核函数的阶数。默认 = 3。\n",
    "    C：正则化参数，默认=1.0。(后面会详细介绍)\n",
    "    epsilon：SVR模型允许的不计算误差的邻域大小。默认0.1。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reverse-delicious",
   "metadata": {},
   "source": [
    "\n",
    "   svm 类中的 SVC() 算法中包含两种核函数：\n",
    "\n",
    "    SVC(kernel = 'ploy')：表示算法使用多项式核函数；\n",
    "    SVC(kernel = 'rbf')：表示算法使用高斯核函数；\n",
    "\n",
    "    SVM 算法的本质就是求解目标函数的最优化问题；"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "greatest-kansas",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "svc = SVC(kernel = 'ploy')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "addressed-hazard",
   "metadata": {},
   "source": [
    "二、高斯核函数（RBF）\n",
    "    业务问题：怎么分类非线性可分的样本的分类？\n",
    "\n",
    "　1）思想\n",
    "\n",
    "    业务的目的是样本分类，采用的方法：按一定规律统一改变样本的特征数据得到新的样本，新的样本按新的特征数据能更好的分类，由于新的样本的特征数据与原始样本的特征数据呈一定规律的对应关系，因此根据新的样本的分布及分类情况，得出原始样本的分类情况。\n",
    "    应该是试验反馈，将样本的特征数据按一定规律统一改变后，同类样本更好的凝聚在了一起；\n",
    "    高斯核和多项式核干的事情截然不同的，如果对于样本数量少，特征多的数据集，高斯核相当于对样本降维；\n",
    "\n",
    "    高斯核的任务：找到更有利分类任务的新的空间。\n",
    "    方法：类似  的映射。\n",
    "    高斯核本质是在衡量样本和样本之间的“相似度”，在一个刻画“相似度”的空间中，让同类样本更好的聚在一起，进而线性可分。\n",
    "\n",
    "    疑问：\n",
    "    “衡量”的手段 ，经过这种映射之后，为什么同类样本能更好的分布在一起？\n",
    " \n",
    "　2）定义方式\n",
    "    x、y：样本或向量；\n",
    "    γ：超参数；高斯核函数唯一的超参数；\n",
    "    || x - y ||：表示向量的范数，可以理解为向量的模；\n",
    "    表示两个向量之间的关系，结果为一个具体值；\n",
    "    高斯核函数的定义公式就是进行点乘的计算公式；\n",
    " \n",
    "　3）功能\n",
    "    先将原始的数据点（x， y）映射为新的样本（x'，y'）；\n",
    "    再将新的特征向量点乘（x' . y'），返回其点乘结果；\n",
    "    计算点积的原因：此处只针对 SVM 中的应用，在其它算法中的应用不一定需要计算点积；\n",
    "　4）特点\n",
    "    高斯核运行开销耗时较大，训练时间较长；\n",
    "    一般使用场景：数据集 (m, n)，m < n；\n",
    "    一般应用领域：自然语言处理；\n",
    "    自然语言处理：通常会构建非常高维的特征空间，但有时候样本数量并不多； \n",
    "　5）高斯函数\n",
    "    正态分布就是一个高斯函数；\n",
    "    高斯函数和高斯核函数，形式类似；\n",
    " \n",
    "　6）其它\n",
    "    高斯核函数，也称为 RBF 核（Radial Basis Function Kernel），也称为径向基函数；\n",
    "    高斯核函数的本质：将每一个样本点映射到一个无穷维的特征空间；\n",
    "    无穷维：将 m*n 的数据集，映射为 m*m 的数据集，m 表示样本个数，n 表示原始样本特征种类，样本个数是无穷的，因此，得到的新的数据集的样本也是无穷维的；\n",
    "    高斯核升维的本质，使得线性不可分的数据线性可分；"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "played-firmware",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAD4CAYAAADhNOGaAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAVYElEQVR4nO3cf0xV5+HH8c+Fq3VwEbn3CgzFtDKbRYu1ipkjMf7gZuvmuhLtnD+2xPlH4+rq0GyxtlrbOpFutViky5rUOWNItSOoc0tMypg20dWC9tqoiRXtolYpci9SL9ry457vH353NwZUDxc81Of9+ovnuc855yPl+PH8oC7LsiwBAIyV4HQAAICzKAIAMBxFAACGowgAwHAUAQAYjiIAAMO5nQ7QV5cvX+7Tdn6/X01NTf2cJn7ksodc9pDLnns1V1ZWVo/zXBEAgOEoAgAwHEUAAIajCADAcBQBABiOIgAAw1EEAGA4igAADEcRAIDhKAIAMBxFAACGowgAwHAUAQAYjiIAAMNRBABgOIoAAAxHEQCA4SgCADAcRQAAhqMIAMBwFAEAGI4iAADDUQQAYDiKAAAMRxEAgOHc/bGTYDCo7du3KxqNqqCgQIWFhV0+b29vV3l5uc6fP6+UlBQVFRUpPT099nlTU5NWrlypH/3oR/rhD3/YH5EAAHco7iuCaDSqbdu26dlnn1VpaakOHz6sS5cudVlTU1Oj5ORkbd26VXPmzFFFRUWXz3fs2KFHHnkk3igAgD6Iuwjq6+uVmZmpjIwMud1u5efnq7a2tsuauro6zZw5U5I0bdo0nTx5UpZlSZLef/99paena/To0fFGAQD0Qdy3hsLhsHw+X2zs8/l09uzZXtckJiYqKSlJ169f19ChQ7Vv3z6tW7dOf/nLX770ONXV1aqurpYklZSUyO/39ymv2+3u87YDiVz2kMsectljWq5+eUbQV2+//bbmzJmjYcOG3XZtIBBQIBCIjZuamvp0TL/f3+dtBxK57CGXPeSy517NlZWV1eN83EXg9XoVCoVi41AoJK/X2+Man8+nzs5O3bhxQykpKaqvr9fRo0dVUVGh1tZWuVwuDR06VI8++mi8sQAAdyjuIsjJydGVK1fU2Ngor9erI0eOaMWKFV3WTJkyRQcPHtSDDz6o9957TxMmTJDL5dJLL70UW/P2229r2LBhlAAA3GVxF0FiYqKWLl2qjRs3KhqNatasWcrOztbu3buVk5OjvLw8zZ49W+Xl5Xr66afl8XhUVFTUD9EBAP2hX54RTJ48WZMnT+4y9+Mf/zj29dChQ7Vq1aov3cf8+fP7IwoAwCZ+sxgADEcRAIDhKAIAMBxFAACGowgAwHAUAQAYjiIAAMNRBABgOIoAAAxHEQCA4SgCADAcRQAAhqMIAMBwFAEAGI4iAADDUQQAYDiKAAAMRxEAgOEoAgAwHEUAAIajCADAcBQBABiOIgAAw1EEAGA4igAADEcRAIDhKAIAMBxFAACGowgAwHAUAQAYzt0fOwkGg9q+fbui0agKCgpUWFjY5fP29naVl5fr/PnzSklJUVFRkdLT0/Xhhx+qoqJCHR0dcrvd+ulPf6qHHnqoPyIBAO5Q3FcE0WhU27Zt07PPPqvS0lIdPnxYly5d6rKmpqZGycnJ2rp1q+bMmaOKigpJUkpKilavXq3Nmzdr+fLl2rp1a7xxAAA2xV0E9fX1yszMVEZGhtxut/Lz81VbW9tlTV1dnWbOnClJmjZtmk6ePCnLsvTAAw/I6/VKkrKzs9XW1qb29vZ4IwEAbIi7CMLhsHw+X2zs8/kUDod7XZOYmKikpCRdv369y5qjR49q7NixGjJkSLyRAAA29MszgnhdvHhRFRUVeu6553pdU11drerqaklSSUmJ/H5/n47ldrv7vO1AIpc95LKHXPaYlivuIvB6vQqFQrFxKBSK3e753zU+n0+dnZ26ceOGUlJSYutfeeUVLV++XJmZmb0eJxAIKBAIxMZNTU19yuv3+/u87UAilz3ksodc9tyrubKysnqcj/vWUE5Ojq5cuaLGxkZ1dHToyJEjysvL67JmypQpOnjwoCTpvffe04QJE+RyudTa2qqSkhItWrRI3/zmN+ONAgDog7ivCBITE7V06VJt3LhR0WhUs2bNUnZ2tnbv3q2cnBzl5eVp9uzZKi8v19NPPy2Px6OioiJJ0oEDB9TQ0KDKykpVVlZKktauXavU1NR4YwEA7pDLsizL6RB9cfny5T5td69e8g0UctlDLnvIZc+gvTUEAPhqowgAwHAUAQAYjiIAAMNRBABgOIoAAAxHEQCA4SgCADAcRQAAhqMIAMBwFAEAGI4iAADDUQQAYDiKAAAMRxEAgOEoAgAwHEUAAIajCADAcBQBABiOIgAAw1EEAGA4igAADEcRAIDhKAIAMBxFAACGowgAwHAUAQAYjiIAAMNRBABgOIoAAAxHEQCA4VyWZVnx7iQYDGr79u2KRqMqKChQYWFhl8/b29tVXl6u8+fPKyUlRUVFRUpPT5ck7dmzRzU1NUpISNDPfvYzTZo06Y6OefnyZVsZo1cbpH0VcrdeV0dyivT4YiWMzLS1j4EwWHM1RNpUcaJJkQ7J45YWP+xXpmeo07HIRa4BNVjPx/7KlZWV1eO8O+6A0ai2bdumtWvXyufzac2aNcrLy9Po0aNja2pqapScnKytW7fq8OHDqqio0MqVK3Xp0iUdOXJEr776qpqbm7Vhwwa99tprSkjo3wuV6NUGWaXPS1cb1P7vyfNnFF35kqP/kQdrroZIm9b//aIaIrFU+qjppl4syHb0ZCUXuQbSYD0f70auuP/Gra+vV2ZmpjIyMuR2u5Wfn6/a2toua+rq6jRz5kxJ0rRp03Ty5ElZlqXa2lrl5+dryJAhSk9PV2Zmpurr6+ON1N2+CulqQ9e5/29YRw3SXBUnmrqcpJLUEGlXxYkmhxLdQi57yGXTID0f70auuK8IwuGwfD5fbOzz+XT27Nle1yQmJiopKUnXr19XOBzWuHHjYuu8Xq/C4XCPx6murlZ1dbUkqaSkRH6//84ztl5Xew/z7tbr8trYT38brLkiHT3fdot0uGx93/sbuewhlz2D9Xy8G7niLoK7JRAIKBAIxMZNTXf+r4dockqP8x3JKbb2098Gay5PLz8VHrdFrh6P39s8uXo+fm/zzuYarOdjf+bq7RlB3LeGvF6vQqFQbBwKheT1entd09nZqRs3biglJaXbtuFwuNu2/eLxxdL/3ksbmXlr3kmDNNetB3dDusxleoZo8cPO/atIIpdd5LJpkJ6PdyNX3G8NdXZ26pe//KWef/55eb1erVmzRitWrFB2dnZszYEDB3ThwgU9+eSTOnz4sI4ePapVq1bp4sWLKisrU3FxsZqbm/XSSy+prKzsjh4W89bQwPrPWx0uedzWoHmrg1zkGkiD9Xwc6LeG+uX10ePHj2vHjh2KRqOaNWuW5s6dq927dysnJ0d5eXlqa2tTeXm5Pv74Y3k8HhUVFSkjI0OSVFVVpX/84x9KSEjQkiVL9Mgjj9zRMe0Wwb/5/X5HL/N6Qy57yGUPuey5V3MNaBE4gSK4O8hlD7nsIZc9A1UE/GYxABiOIgAAw1EEAGA4igAADEcRAIDhKAIAMBxFAACGowgAwHAUAQAYjiIAAMNRBABgOIoAAAxHEQCA4SgCADAcRQAAhqMIAMBwFAEAGI4iAADDUQQAYDiKAAAMRxEAgOEoAgAwHEUAAIajCADAcBQBABiOIgAAw1EEAGA4igAADEcRAIDhKAIAMJw7no0jkYhKS0t19epVjRw5UitXrpTH4+m27uDBg6qqqpIkzZ07VzNnztQXX3yhV199VZ9++qkSEhI0ZcoULV68OJ44AIA+iOuKYO/evcrNzVVZWZlyc3O1d+/ebmsikYgqKytVXFys4uJiVVZWKhKJSJIee+wxbdmyRb/97W915swZffDBB/HEAQD0QVxFUFtbqxkzZkiSZsyYodra2m5rgsGgJk6cKI/HI4/Ho4kTJyoYDOq+++7TQw89JElyu9164IEHFAqF4okDAOiDuIqgpaVFaWlpkqQRI0aopaWl25pwOCyfzxcbe71ehcPhLmtaW1t17Ngx5ebmxhMHANAHt31GsGHDBl27dq3b/IIFC7qMXS6XXC6X7QCdnZ167bXX9L3vfU8ZGRm9rquurlZ1dbUkqaSkRH6/3/axpFtXH33ddiCRyx5y2UMue0zLddsiWLduXa+fpaamqrm5WWlpaWpubtbw4cO7rfF6vTp9+nRsHA6HNX78+Nj4jTfeUGZmpubMmfOlOQKBgAKBQGzc1NR0u+g98vv9fd52IJHLHnLZQy577tVcWVlZPc7HdWsoLy9Phw4dkiQdOnRIU6dO7bZm0qRJOnHihCKRiCKRiE6cOKFJkyZJknbt2qUbN25oyZIl8cQAAMQhrtdHCwsLVVpaqpqamtjro5J07tw5vfPOO1q2bJk8Ho/mzZunNWvWSJKeeOIJeTwehUIhVVVVadSoUVq9erUk6dFHH1VBQUGcfyQAgB0uy7Isp0P0xeXLl/u03b16yTdQyGUPuewhlz2D8tYQAOCrjyIAAMNRBABgOIoAAAxHEQCA4SgCADAcRQAAhqMIAMBwFAEAGI4iAADDUQQAYDiKAAAMRxEAgOEoAgAwHEUAAIajCADAcBQBABiOIgAAw1EEAGA4igAADEcRAIDhKAIAMBxFAACGowgAwHAUAQAYjiIAAMNRBABgOIoAAAxHEQCA4SgCADAcRQAAhnPHs3EkElFpaamuXr2qkSNHauXKlfJ4PN3WHTx4UFVVVZKkuXPnaubMmV0+f/nll9XY2KjNmzfHEwcA0AdxXRHs3btXubm5KisrU25urvbu3dttTSQSUWVlpYqLi1VcXKzKykpFIpHY50ePHtWwYcPiiQEAiENcRVBbW6sZM2ZIkmbMmKHa2tpua4LBoCZOnCiPxyOPx6OJEycqGAxKkj7//HP99a9/1bx58+KJAQCIQ1y3hlpaWpSWliZJGjFihFpaWrqtCYfD8vl8sbHX61U4HJYk7dq1S4899piGDh1622NVV1erurpaklRSUiK/39+nzG63u8/bDiRy2UMue8hlj2m5blsEGzZs0LVr17rNL1iwoMvY5XLJ5XLd8YH/9a9/6dNPP9WSJUvU2Nh42/WBQECBQCA2bmpquuNj/Te/39/nbQcSuewhlz3ksudezZWVldXj/G2LYN26db1+lpqaqubmZqWlpam5uVnDhw/vtsbr9er06dOxcTgc1vjx4/XRRx/p/PnzWr58uTo7O9XS0qIXXnhBL7zwwh38cQAA/SWuW0N5eXk6dOiQCgsLdejQIU2dOrXbmkmTJumtt96KPSA+ceKEFi1aJI/Ho+985zuSpMbGRr388suUAAA4IK4iKCwsVGlpqWpqamKvj0rSuXPn9M4772jZsmXyeDyaN2+e1qxZI0l64oknenzFFADgDJdlWZbTIfri8uXLfdruXr33N1DIZQ+57CGXPQP1jIDfLAYAw1EEAGA4igAADEcRAIDhKAIAMBxFAACGowgAwHAUAQAYjiIAAMNRBABgOIoAAAxHEQCA4SgCADAcRQAAhqMIAMBwFAEAGI4iAADDUQQAYDiKAAAMRxEAgOEoAgAwHEUAAIajCADAcBQBABjOZVmW5XQIAIBzjLsieOaZZ5yO0CNy2UMue8hlj2m5jCsCAEBXFAEAGM64IggEAk5H6BG57CGXPeSyx7RcPCwGAMMZd0UAAOiKIgAAw7mdDuCk/fv3a+fOnXrzzTc1fPhwp+No165dqqurk8vlUmpqqp566il5vV6nY2nnzp06duyY3G63MjIy9NRTTyk5OdnpWPrnP/+pP//5z/rkk09UXFysnJwcR/MEg0Ft375d0WhUBQUFKiwsdDSPJP3+97/X8ePHlZqaqs2bNzsdJ6apqUmvv/66rl27JpfLpUAgoO9///tOx1JbW5vWr1+vjo4OdXZ2atq0aZo/f77TsWKi0aieeeYZeb3e/n2V1DLU1atXrd/85jfWz3/+c6ulpcXpOJZlWVZra2vs67/97W/WG2+84WCa/wgGg1ZHR4dlWZa1c+dOa+fOnQ4nuuXixYvWJ598Yq1fv96qr693NEtnZ6f1i1/8wmpoaLDa29utX/3qV9bFixcdzWRZlnXq1Cnr3Llz1qpVq5yO0kU4HLbOnTtnWZZl3bhxw1qxYsWg+H5Fo1Hr5s2blmVZVnt7u7VmzRrrzJkzDqf6j/3791tbtmyxNm3a1K/7NfbW0I4dO7R48WK5XC6no8QkJSXFvv7iiy8GTbaHH35YiYmJkqQHH3xQ4XDY4US3jB49WllZWU7HkCTV19crMzNTGRkZcrvdys/PV21trdOxNH78eHk8HqdjdJOWlqaxY8dKkr72ta9p1KhRg+LnyuVyadiwYZKkzs5OdXZ2DprzMBQK6fjx4yooKOj3fRt5a6i2tlZer1f333+/01G6eeutt/Tuu+8qKSlJ69evdzpONzU1NcrPz3c6xqATDofl8/liY5/Pp7NnzzqY6KujsbFRH3/8sb7xjW84HUXSrdsvq1evVkNDg7773e9q3LhxTkeSJP3pT3/ST37yE928ebPf933PFsGGDRt07dq1bvMLFizQnj17tHbt2rsfSl+ea+rUqVq4cKEWLlyoPXv26MCBA3ft/uTtcklSVVWVEhMTNX369LuS6U5z4avr888/1+bNm7VkyZIuV8ROSkhI0O9+9zu1trbqlVde0YULFzRmzBhHMx07dkypqakaO3asTp061e/7v2eLYN26dT3OX7hwQY2Njfr1r38t6dbl1urVq7Vp0yaNGDHCsVz/a/r06dq0adNdK4Lb5Tp48KCOHTum559//q5eKt/p98tpXq9XoVAoNg6FQoPiQf9g1tHRoc2bN2v69On61re+5XScbpKTkzVhwgQFg0HHi+DMmTOqq6vTBx98oLa2Nt28eVNlZWVasWJFv+z/ni2C3owZM0ZvvvlmbLx8+XJt2rRpULw1dOXKFX3961+XdOv21WC5/x0MBrVv3z69+OKLuu+++5yOMyjl5OToypUramxslNfr1ZEjR/rtJL0XWZalP/zhDxo1apR+8IMfOB0n5rPPPlNiYqKSk5PV1tamDz/8UI8//rjTsbRo0SItWrRIknTq1Cnt37+/X3++jCuCwayiokJXrlyRy+WS3+/Xk08+6XQkSdK2bdvU0dGhDRs2SJLGjRs3KLK9//77+uMf/6jPPvtMJSUluv/++/Xcc885kiUxMVFLly7Vxo0bFY1GNWvWLGVnZzuS5b9t2bJFp0+f1vXr17Vs2TLNnz9fs2fPdjqWzpw5o3fffVdjxoyJXZ0vXLhQkydPdjRXc3OzXn/9dUWjUVmWpW9/+9uaMmWKo5nuBv4XEwBgOGNfHwUA3EIRAIDhKAIAMBxFAACGowgAwHAUAQAYjiIAAMP9HytN4DVGjIwvAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = np.arange(-4, 5, 1)\n",
    "y = np.array((x >= -2) & (x <= 2), dtype='int')\n",
    "\n",
    "plt.scatter(x[y==0], [0]*len(x[y==0]))\n",
    "plt.scatter(x[y==1], [0]*len(x[y==1]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "mathematical-liabilities",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "def gaussian(x, l):\n",
    "    # 此处直接将超参数 γ 设定为 1.0；\n",
    "    # 此处 x 表示一维的样本，也就是一个具体的值，l 相应的也是一个具体的数，因为 l 和 x 一样，从特征空间中选定；\n",
    "    gamma = 1.0\n",
    "    # 此处因为 x 和 l 都只是一个数，不需要再计算模，可以直接平方；\n",
    "    return np.exp(-gamma * (x-l)**2)\n",
    "\n",
    "# 设定地标 l1、l2 为 -1和1\n",
    "l1, l2 = -1, 1\n",
    "x_new = np.empty((len(x), 2))\n",
    "\n",
    "for i, data in enumerate(x):\n",
    "    x_new[i, 0] = gaussian(data, l1)\n",
    "    x_new[i, 1] = gaussian(data, l2)\n",
    "\n",
    "plt.scatter(x_new[y==0, 0], x_new[y==0, 1])\n",
    "plt.scatter(x_new[y==1, 0], x_new[y==1, 1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "declared-trance",
   "metadata": {},
   "source": [
    "在机器学习中，当所学问题有了具体的形式之后，机器学习就会形式化为一个求优化的问题。不论是梯度下降法、随机梯度下降、牛顿法、拟牛顿法，抑或是 Adam 之类的高级的优化算法，这些都需要花时间掌握去掌握其数学原理。 \n",
    "![jupyter](./640.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quality-softball",
   "metadata": {},
   "source": [
    "Adam如何工作？\n",
    "\n",
    "Adam与经典的随机梯度下降法是不同的。随机梯度下降保持一个单一的学习速率(称为alpha)，用于所有的权重更新，并且在训练过程中学习速率不会改变。每一个网络权重(参数)都保持一个学习速率，并随着学习的展开而单独地进行调整。该方法从梯度的第一次和第二次矩的预算来计算不同参数的自适应学习速率。\n",
    "\n",
    "作者描述Adam时将随机梯度下降法两种扩展的优势结合在一起。\n",
    "\n",
    "具体地说:\n",
    "\n",
    "自适应梯度算法(AdaGrad)维护一个参数的学习速率，可以提高在稀疏梯度问题上的性能(例如，自然语言和计算机视觉问题)。\n",
    "\n",
    "均方根传播(RMSProp)也维护每个参数的学习速率，根据最近的权重梯度的平均值(例如变化的速度)来调整。这意味着该算法在线上和非平稳问题上表现良好(如:噪声)。\n",
    "\n",
    "Adam意识到AdaGrad和RMSProp的好处。与在RMSProp中基于平均第一个时刻(平均值)的参数学习速率不同，Adam也使用了梯度的第二个时刻的平均值(非中心方差)。\n",
    "\n",
    "具体地说，该算法计算了梯度和平方梯度的指数移动平均值，并且参数beta1和beta2控制了这些移动平均的衰减率。移动平均值和beta1和beta2的初始值接近1.0(推荐值)，这导致了估计时间的偏差为0。这种偏差是通过第一次计算偏差估计然后再计算比可用偏差校正估计来克服的。\n",
    "Adam是有效的\n",
    "\n",
    "Adam在深度学习领域是一种很受欢迎的算法，因为它能很快取得好的成果。实证结果表明：在实践中，Adam的工作表现良好，并优于其他随机优化方法。\n",
    "\n",
    "在原论文中，通过实证证明了该方法的收敛性满足了理论分析的期望。\n",
    "\n",
    "在MNIST的字符识别和IMDB情绪分析数据集上，Adam采用了逻辑回归算法，对MNIST的数据集进行了多层感知器算法，并在CIFAR-10图像识别数据集上进行了卷积神经网络。\n",
    "\n",
    "他们的结论是:\n",
    "\n",
    "  “利用大型模型和数据集，我们证明了Adam可以有效地解决实际的深度学习问题。”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "complimentary-exercise",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "参数为：\n",
      " [ 2.25328063e+01 -9.28034172e-01  1.08137159e+00  1.40288639e-01\n",
      "  6.81827192e-01 -2.05661202e+00  2.67434112e+00  1.93722011e-02\n",
      " -3.10408881e+00  2.66067513e+00 -2.07502796e+00 -2.06054968e+00\n",
      "  8.49257551e-01 -3.74356764e+00]\n",
      "loss为：\n",
      " 21.894831568022624\n",
      "target：\n",
      " [24.  21.6 34.7 33.4 36.2]\n",
      "target_predict：\n",
      " [30.00473451 25.02539018 30.56756258 28.6070556  27.943559  ]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.datasets import load_boston\n",
    "class Gradient_Descent():\n",
    "    def __init__(self):\n",
    "        self.data_x = data_x\n",
    "        self.data_y = data_y\n",
    "        self.feature = data_features\n",
    "        \n",
    "    def data_clearning(self):\n",
    "        rows = self.data_x.shape[0]\n",
    "        data_mean = np.mean(self.data_x,axis=0)\n",
    "        data_std = np.std(self.data_x,axis=0)\n",
    "        self.x_std = (self.data_x-data_mean) / data_std\n",
    "        I = np.zeros(rows)+1.\n",
    "        self.x_b_std = np.insert(self.x_std,0,I,axis=1)\n",
    "        return self.x_b_std\n",
    "    \n",
    "    def dLoss(self,theta):\n",
    "        return self.x_b_std.T.dot(self.x_b_std.dot(theta)-self.data_y)*2/self.x_b_std.shape[0]\n",
    "    \n",
    "    \n",
    "    def Loss(self,theta):\n",
    "        return (self.x_b_std.dot(theta)-self.data_y).T.dot(self.x_b_std.dot(theta)-self.data_y)/self.x_b_std.shape[0]\n",
    "    \n",
    "    def gradient_descent(self):\n",
    "        theta = np.zeros(self.x_b_std.shape[1])\n",
    "        eta = 0.1\n",
    "        epsilon = 1e-8\n",
    "        loss = []\n",
    "        while True:\n",
    "            dL = self.dLoss(theta)\n",
    "            loss.append(np.sum(np.abs(self.Loss(theta))))\n",
    "            last_theta = theta\n",
    "            theta = theta - eta * dL\n",
    "            if np.abs(np.sum(np.abs(self.Loss(theta))-np.sum(np.abs(self.Loss(last_theta)))))<epsilon:\n",
    "                break\n",
    "        return theta,loss\n",
    "    def plot_loss(self,loss):\n",
    "        plt.style.use(\"ggplot\")\n",
    "        plt.title(\"Loss\")\n",
    "        plt.xlim(-1,100)\n",
    "        plt.xlabel(\"times\")\n",
    "        plt.plot(loss)\n",
    "        \n",
    "    def predict(self,theta,x_std):\n",
    "        return x_std.dot(theta.T)\n",
    "  \n",
    "    def start(self):\n",
    "        self.data_clearning()\n",
    "        theta,loss = self.gradient_descent()\n",
    "        self.plot_loss(loss)\n",
    "        predict = self.predict(theta,self.x_b_std)\n",
    "        return theta,loss,predict\n",
    "if __name__ == '__main__':\n",
    "    boston = load_boston()\n",
    "    data_x = boston.data\n",
    "    data_y = boston.target\n",
    "    data_features = boston.feature_names\n",
    "    GD = Gradient_Descent()\n",
    "    theta,loss,predict = GD.start()\n",
    "    print(\"参数为：\\n\",theta)\n",
    "    print(\"loss为：\\n\",loss[-1])\n",
    "    print(\"target：\\n\",data_y[:5])\n",
    "    print(\"target_predict：\\n\",predict[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "abstract-numbers",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "参数为：\n",
      " [22.47801577 -1.02362669  1.24340697  0.63328489  0.67433268 -2.26413982\n",
      "  2.56079225  0.11422948 -3.15602355  4.1443037  -3.67599279 -2.04863329\n",
      "  0.876291   -3.77317089]\n",
      "loss为：\n",
      " 22.23940461259748\n",
      "target：\n",
      " [24.  21.6 34.7 33.4 36.2]\n",
      "target_predict：\n",
      " [29.00304379 25.09438571 30.47292279 28.5026782  27.83376006]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.datasets import load_boston\n",
    "class SGD():\n",
    "    def __init__(self):\n",
    "        self.data_x = data_x\n",
    "        self.data_y = data_y\n",
    "        self.feature = data_features\n",
    "        \n",
    "    def data_clearning(self):\n",
    "        rows = self.data_x.shape[0]\n",
    "        data_mean = np.mean(self.data_x,axis=0)\n",
    "        data_std = np.std(self.data_x,axis=0)\n",
    "        self.x_std = (self.data_x-data_mean) / data_std\n",
    "        I = np.zeros(rows)+1.\n",
    "        self.x_b_std = np.insert(self.x_std,0,I,axis=1)\n",
    "        return self.x_b_std\n",
    "    \n",
    "    def dLoss(self,theta,i):\n",
    "        return self.x_b_std[i,:].T.dot(self.x_b_std[i,:].dot(theta)-self.data_y[i])*2\n",
    "    \n",
    "    \n",
    "    def Loss(self,theta):\n",
    "        return (self.x_b_std.dot(theta)-self.data_y).T.dot(self.x_b_std.dot(theta)-self.data_y)/self.x_b_std.shape[0]\n",
    "    \n",
    "    def gradient_descent(self):\n",
    "        theta = np.zeros(self.x_b_std.shape[1])\n",
    "        t0 = 5\n",
    "        t1 = 50\n",
    "        epsilon = 1e-8\n",
    "        loss = []\n",
    "        for i in range(self.x_b_std.shape[0]*100):\n",
    "            eta = t0/(i + t1)\n",
    "            i_int = np.random.randint(self.x_b_std.shape[0])\n",
    "            dL = self.dLoss(theta,i_int)\n",
    "            loss.append(np.sum(np.abs(self.Loss(theta))))\n",
    "            last_theta = theta\n",
    "            theta = theta - eta * dL\n",
    "            \n",
    "        return theta,loss\n",
    "    def plot_loss(self,loss):\n",
    "        plt.style.use(\"ggplot\")\n",
    "        plt.title(\"SGD_Loss\")\n",
    "        plt.xlim(-1,1000)\n",
    "        plt.xlabel(\"times\")\n",
    "        plt.plot(loss)\n",
    "        \n",
    "    def predict(self,theta,x_std):\n",
    "        return x_std.dot(theta.T)\n",
    "  \n",
    "    def start(self):\n",
    "        self.data_clearning()\n",
    "        theta,loss = self.gradient_descent()\n",
    "        self.plot_loss(loss)\n",
    "        predict = self.predict(theta,self.x_b_std)\n",
    "        return theta,loss,predict\n",
    "if __name__ == '__main__':\n",
    "    boston = load_boston()\n",
    "    data_x = boston.data\n",
    "    data_y = boston.target\n",
    "    data_features = boston.feature_names\n",
    "    sgd = SGD()\n",
    "    theta,loss,predict = sgd.start()\n",
    "    print(\"参数为：\\n\",theta)\n",
    "    print(\"loss为：\\n\",loss[-1])\n",
    "    print(\"target：\\n\",data_y[:5])\n",
    "    print(\"target_predict：\\n\",predict[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "amazing-masters",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "参数为：\n",
      " [22.45738299 -0.8726229   1.0874557   0.23976732  0.64053255 -2.10757243\n",
      "  2.86006123  0.09480008 -3.05482278  2.59488461 -2.13892698 -2.11151337\n",
      "  0.88917914 -3.69710995]\n",
      "loss为：\n",
      " 21.944993904312792\n",
      "target：\n",
      " [24.  21.6 34.7 33.4 36.2]\n",
      "target_predict：\n",
      " [30.02220551 25.13541519 30.79692973 28.67795973 28.09321185]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.datasets import load_boston\n",
    "class Adam():\n",
    "    def __init__(self):\n",
    "        self.data_x = data_x\n",
    "        self.data_y = data_y\n",
    "        self.feature = data_features\n",
    "        \n",
    "    def data_clearning(self):\n",
    "        rows = self.data_x.shape[0]\n",
    "        data_mean = np.mean(self.data_x,axis=0)\n",
    "        data_std = np.std(self.data_x,axis=0)\n",
    "        self.x_std = (self.data_x-data_mean) / data_std\n",
    "        I = np.zeros(rows)+1.\n",
    "        self.x_b_std = np.insert(self.x_std,0,I,axis=1)\n",
    "        return self.x_b_std\n",
    "    \n",
    "    def dLoss(self,theta,i):\n",
    "        return self.x_b_std[i,:].T.dot(self.x_b_std[i,:].dot(theta)-self.data_y[i])*2\n",
    "    \n",
    "    \n",
    "    def Loss(self,theta):\n",
    "        return (self.x_b_std.dot(theta)-self.data_y).T.dot(self.x_b_std.dot(theta)-self.data_y)/self.x_b_std.shape[0]\n",
    "    \n",
    "    def gradient_descent(self):\n",
    "        theta = np.zeros(self.x_b_std.shape[1])\n",
    "        t0 = 5\n",
    "        t1 = 50\n",
    "        beta1 = 0.9\n",
    "        beta2 = 0.999\n",
    "        mt = np.zeros(self.x_b_std.shape[1])\n",
    "        epsilon = 1e-8\n",
    "        loss = []\n",
    "        for i in range(self.x_b_std.shape[0]*1000):\n",
    "            eta =  0.01\n",
    "            i_int = np.random.randint(self.x_b_std.shape[0])\n",
    "            dL = self.dLoss(theta,i_int)\n",
    "            if i==0:\n",
    "                vt=dL**2\n",
    "            vt = beta2*vt+(1-beta2)*(dL**2)\n",
    "            mt = beta1*mt + (1-beta1)*dL\n",
    "            loss.append(np.sum(np.abs(self.Loss(theta))))\n",
    "            last_theta = theta\n",
    "            theta = theta - eta * (mt/(1-beta1))/(np.sqrt(vt/(1-beta2))+epsilon)\n",
    "            \n",
    "        return theta,loss\n",
    "    def plot_loss(self,loss):\n",
    "        plt.style.use(\"ggplot\")\n",
    "        plt.title(\"Adam_Loss\")\n",
    "        #plt.xlim(-1,100)\n",
    "        plt.xlabel(\"times\")\n",
    "        plt.plot(loss)\n",
    "        \n",
    "    def predict(self,theta,x_std):\n",
    "        return x_std.dot(theta.T)\n",
    "  \n",
    "    def start(self):\n",
    "        self.data_clearning()\n",
    "        theta,loss = self.gradient_descent()\n",
    "        self.plot_loss(loss)\n",
    "        predict = self.predict(theta,self.x_b_std)\n",
    "        return theta,loss,predict\n",
    "if __name__ == '__main__':\n",
    "    boston = load_boston()\n",
    "    data_x = boston.data\n",
    "    data_y = boston.target\n",
    "    data_features = boston.feature_names\n",
    "    adam = Adam()\n",
    "    theta,loss,predict = adam.start()\n",
    "    print(\"参数为：\\n\",theta)\n",
    "    print(\"loss为：\\n\",loss[-1])\n",
    "    print(\"target：\\n\",data_y[:5])\n",
    "    print(\"target_predict：\\n\",predict[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "overall-principal",
   "metadata": {},
   "source": [
    "3、模型评估：\n",
    "\n",
    "       （1）均方误差 MSE/RMSE（Mean Squared Error）\n",
    "\n",
    "           MSE指的是参数估计值与实际值之差的平方的期望值，这个值越小说明预测模型具有越好的精确度。RMSE是MSE的算术平方根。                \n",
    "\n",
    "           使用方法：from sklearn.metrics import mean_squared_error\n",
    "\n",
    "        （2）R2值（R-Squared/Coefficient of determination）\n",
    "\n",
    "            拟合优度/决定系数R2∈(-∞,1]指回归直线对观测值的拟合程度，即将预测值与只使用均值的情况相比，值越接近于1说明回归直线对观测值的拟合程度越好。\n",
    "\n",
    "            线性回归模型一般用R-Squared评价模型的表现。               \n",
    "\n",
    "            使用方法：from sklearn.metrics import r2_score \n",
    "\n",
    "训练集与测试集的划分\n",
    "\n",
    "    1、留出法（hold-out）：\n",
    "\n",
    "        随机将初始数据集一部分划分为训练集，剩余部分作为测试集，划分时应尽量保证数据分布的一致性，一般测试集的数量少于原样本数量的三分之一左右。 \n",
    "\n",
    "    2、交叉验证法（k-fold cross validation）：\n",
    "\n",
    "        将初始数据集随机划分为k个互斥子集，用k-1个子集作为训练集，剩下一个作为测试集，轮流将每一个子集作为测试集，一共进行k次建模，最终得到测试结果的均值（score的平均得分）。k值一般为5、10。\n",
    "\n",
    "        将数据集随机划分为k个互斥子集，一共进行p次这样的操作，最后对p个k-fold cv进行取平均，称为p次k折交叉验证。\n",
    "\n",
    "        交叉验证法的目的：校准当前模型的准确率score（注意和GridCV的区别） \n",
    "\n",
    "    3、留一法LOOCV（Leave-one-out cross validation）：\n",
    "\n",
    "        若数据集一共有m个样本，随机令一条数据作为测试数据，其他作为训练数据，这种划分方法是交叉验证法的特殊情况。这种划分方法的优势是，每个模型都能很好的反应原始数据集的特性；缺点是计算量在数据量大的时候会非常大，还不算调超参的计算量。\n",
    "    4、自助法（Bootstrapping）：\n",
    "\n",
    "        对数据集D中的m个数据进行有放回的随机取样，重复m次，产生一个新的数据集D’,将未被取到的数据集作为测试集。数据从未被取到的的几率约为36.8%。\n",
    "注意：模型的好坏和训练集、测试集的划分无关，而是和参数、算法选择等有关，因此不必过于纠结划分方式的选择。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "genuine-island",
   "metadata": {},
   "source": [
    "正则化：\n",
    "为了防止线性回归中出现过拟合现象，我们通常采用正则化方法进行处理(在损失函数中添加一个正则项)。\n",
    "接下来，用以下标记来描述这个损失函数:\n",
    "m 表示训练集中实例的数量。𝜆表示用来控制的是对模型正则化的程度。r表示通过控制r来调节岭回归与套索回归的混合比例。\n",
    "岭(Ridge)回归的损失函数：\n",
    "$$J(\\theta)=\\frac{1}{2 m} \\sum_{i=1}^{m}\\left(h_{\\theta}\\left(x^{(i)}\\right)-y^{(i)}\\right)^{2}+\\lambda \\frac{1}{2} \\sum_{j=1}^{n} \\theta_{j}^{2}$$\n",
    "套索(Lasso)回归的损失函数：\n",
    "$$J(\\theta)=\\frac{1}{2 m} \\sum_{i=1}^{m}\\left(h_{\\theta}\\left(x^{(i)}\\right)-y^{(i)}\\right)^{2}+\\lambda \\sum_{j=1}^{n}\\left|\\theta_{j}\\right|$$\n",
    "弹性网络(Elastic Net)的损失函数：\n",
    "$$J(\\theta)=\\frac{1}{2 m} \\sum_{i=1}^{m}\\left(h_{\\theta}\\left(x^{(i)}\\right)-y^{(i)}\\right)^{2}+r \\lambda \\sum_{j=1}^{n}\\left|\\theta_{j}\\right|+\\frac{1-r}{2} \\lambda \\sum_{j=1}^{n} \\theta_{j}^{2}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tropical-contents",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "protective-heritage",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
